[
    {
    "status": "down",
    "affectedServices": "Notebook GPU Kernels",
    "summary": "Unable to launch GPU Notebooks",
    "date": "2025-04-16T17:00",
    "updates": [
      {
        "date": "2025-04-17T09:00",
        "details": "Notebook users are unable to start a Notebook Kernel on the GPU Node, the two other kernels remain available."
      }
    ]
  },
  {
    "status": "down",
    "affectedServices": "LOTUS (old cluster)",
    "summary": "Old cluster now retired for general use",
    "date": "2025-03-28T17:00",
    "updates": [
      {
        "date": "2025-03-28T17:00",
        "details": "As per recent announcements, the old cluster is now retired for general use. The new cluster is available for all users and all sci machines now submit to it."
      }
    ]
  },
  {
    "status": "down",
    "affectedServices": "All CEDA and JASMIN services",
    "summary": "Regular scheduled maintenance day",
    "date": "2025-04-29T00:01",
    "updates": [
      {
        "date": "2025-02-28T13:00",
        "details": "Advance notice of regular scheduled maintenance day on Tues 29 April 2025. Please plan your work to minimise disruption."
      }
    ]
  },
  {
    "status": "resolved",
    "affectedServices": "ORCHID GPU cluster",
    "summary": "ORCHID GPU cluster (batch nodes and interactive node) down for maintenance.",
    "date": "2025-04-02T00:01",
    "updates": [
      {
        "date": "2025-04-01T14:00",
        "details": "[POSTPONED] As part of work to move GPU nodes to Rocky 9 and a different part of the network, batch and interactive GPU nodes will be down for maintenance on [new date TBC]."
      }
    ]
  },
  {
    "status": "degraded",
    "affectedServices": "PFS storage (/work/scratch-pw*, /work/xfc/*, /gws/pw/j07/*)",
    "summary": "I/O errors writing to PFS from within Slurm jobs",
    "date": "2025-04-03T13:00",
    "updates": [
      {
        "date": "2025-04-03T13:00",
        "details": "Issue reported where I/O errors occur when writing to PFS storage volumes from Slurm jobs. Appears to be memory-related: initial advice is to increase memory allocation for job. Investigation underway with vendor, awaiting further advice."
      }
    ]
  },
  {
    "status": "at risk",
    "affectedServices": "SSD storage: HOME, /gws/smf/*, /work/scratch-nopw2",
    "summary": "Scheduled maintenance of SSD storage for software upgrade",
    "date": "2025-04-22T12:00",
    "updates": [
      {
        "date": "2025-04-14T15:00",
        "details": "Advance notice: SSD storage at-risk on afternoon of Tuesday 22nd April 2025. Disruption possible though unlikely, but would have wide-reaching impact, affecting user home directories, /work/scratch-nopw2 and GWS 'SMF' volumes with paths /gws/smf/*."
      }
    ]
  },
  {
    "status": "at risk",
    "affectedServices": "All CEDA and JASMIN services",
    "summary": "Loss of networking at RAL",
    "date": "2025-04-15T10:00",
    "updates": [
      {
        "date": "2025-04-15T10:00",
        "details": "We are aware of a loss of networking at the RAL site where JASMIN is hosted. This is causing problems with external access to JASMIN and CEDA services. The network team has been made aware of the problem, and are working towards a fix."
      }
    ]
  },
  {
    "status": "degraded",
    "affectedServices": "LOTUS2 (new cluster)",
    "summary": "Issue with Slurm controller",
    "date": "2025-04-15T19:30",
    "updates": [
      {
        "date": "2025-04-16T09:00",
        "details": "There was an issue with Slurm controller service 19:30 - 20:30 last night. Users would have experienced Slurm commands hanging, and may not have been able to submit jobs. It is probable that running jobs may have failed/timed out. Possible residual failures/time outs this morning. Slurm controller service has been restarted and jobs are running, Slurm commands running as expected. Further support actions in progress."
      }
    ]
  }
]
