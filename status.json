[
  {
    "status": "degraded",
    "affectedServices": "LOTUS (old cluster)",
    "summary": "Hosts now being retired from old cluster",
    "date": "2025-01-28T17:00",
    "updates": [
      {
        "date": "2025-01-28T17:00",
        "details": "Expect jobs to run more slowly on old LOTUS cluster now ahead of retirement 18 Feb. Please use new LOTUS2 cluster.",
        "url": "https://www.ceda.ac.uk/news/updates/2025/2025-01-28-jasmin-updates-new-cluster-scratch-conda-removal/"
      }
    ]
  },
  {
    "status": "degraded",
    "affectedServices": "LOTUS (old cluster)",
    "summary": "sci-vm hosts submit to new cluster & standard partitions close Tues 18 Feb",
    "date": "2025-02-18T08:00",
    "updates": [
      {
        "date": "2025-02-17T09:00",
        "details": "REMINDER: sci-vm hosts submit to new cluster from Tues 18 Feb, standard partitions on old cluster will close.",
        "url": "https://www.ceda.ac.uk/news/updates/2025/2025-02-07-jasmin-further-updates-new-cluster/"
      },
      {
        "date": "2025-02-18T17:00",
        "details": "Closure of partitions on old cluster postponed while issue with new cluster is ongoing."
      }
    ]
  },
  {
    "status": "degraded",
    "affectedServices": "LOTUS2 (new cluster)",
    "summary": "Issue over weekend, limited hosts available for processing",
    "date": "2025-02-17T09:00",
    "updates": [
      {
        "date": "2025-02-17T09:00",
        "details": "An issue occurred over the weekend with the new cluster: investigation ongoing. Although it is now possible to submit jobs again, fewer hosts than normal are currently available for processing and remains at risk until further notice."
      },
      {
        "date": "2025-02-17T12:00",
        "details": "Investigation ongoing: partitions accepting jobs but not executing. Shutdown deadlines for old LOTUS will be extended by 48hrs to compensate. Expect disruption until further notice."
      },
      {
        "date": "2025-02-18T10:30",
        "details": "Paritions/Queues were briefly reopened earlier but issue recurred: standard and debug partitions now closed again, while investigation continues. Highres partition currently unaffected but PLEASE DO NOT transfer non-highres jobs to that partition. Further details expected later today, apologies for inconvenience."
      }
    ]
  },
  {
    "status": "degraded",
    "affectedServices": "sci, lotus",
    "summary": "Use of watch / --iterate causing excess resource usage",
    "date": "2025-02-18T16:30",
    "updates": [
        {
          "date": "2025-02-18T16:30",
          "details": "Please refrain from using the 'watch' command, and/or the '--iterate' option with Slurm commands. Doing so can overload the Slurm controller in a multi-user environment. Please find an alternative way to monitor your jobs that does not involve contacting the controller repeatedly. Running 'watch' elsewhere must only be done with care, supervision and for a limited duration, since repeated execution of some commands can be extremely wasteful and cause system instability."
        }
      ]
  }
]
